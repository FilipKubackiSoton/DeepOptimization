{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DenseTranspose(tf.keras.layers.Layer):\n",
    "    def __init__(self, dense, activation = None, **kwargs):\n",
    "        self.dense = dense \n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        super(DenseTranspose, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.b = self.add_weight(name= \"bias\", shape = [ self.dense.input_shape[-1]], initializer = \"zeros\")\n",
    "        self.w = self.dense.weights[0]\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        z = tf.linalg.matmul(inputs, self.w, transpose_b = True)\n",
    "        return self.activation(z + self.b)\n",
    "\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {\"w\": np.shape(tf.transpose(self.w))}    \n",
    "\n",
    "    @property \n",
    "    def weights_transpose(self):\n",
    "        return tf.transpose(self.dense.weights[0])\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, input_shape, dropout, reg_cof, compression, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self._output_size = int(input_shape * compression)\n",
    "        self.input_encoder =  Input(shape = (input_shape,))\n",
    "        self.drop_encoder = Dropout(dropout)\n",
    "        self.dense_encoder = Dense(self._output_size,activation=\"tanh\",kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "        bias_initializer=tf.keras.initializers.Zeros(),kernel_regularizer=tf.keras.regularizers.l1(reg_cof))\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.drop_encoder(inputs) \n",
    "        x = self.dense_encoder(x)\n",
    "        return x\n",
    "\n",
    "    def get_dense_encoder(self):\n",
    "        return self.dense_encoder\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._output_size \n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, encoder, **kwargs ):\n",
    "        self.encoder_d = encoder.get_dense_encoder() \n",
    "        self.dense_decoder = DenseTranspose(dense = self.encoder_d)\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.dense_decoder.build(input_shape)\n",
    "        super().build(input_shape)\n",
    "        print(self.dense_decoder.get_weights())\n",
    "\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_decoder(inputs)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, input_size=32, compression = 0.8, dropout = 0.2, reg_cof = 0.001, **kwargs):\n",
    "        super(Autoencoder, self).__init__(**kwargs)\n",
    "        self.input_size = input_size\n",
    "        self.compression = 0.8\n",
    "        self.dropout = dropout\n",
    "        self.reg_cof = reg_cof \n",
    "        encoder = Encoder(\n",
    "            input_shape = self.input_size,\n",
    "            dropout = self.dropout, \n",
    "            reg_cof = self.reg_cof,\n",
    "            compression = self.compression, \n",
    "            )\n",
    "        encoder.build(input_size)\n",
    "        encoder((np.random.rand(32)/1000,))\n",
    "        print(encoder.weights)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = Decoder(\n",
    "            encoder = self.encoder\n",
    "            ) \n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def encode(self, X):\n",
    "        return self.encoder(X)\n",
    "\n",
    "    def decode(self, Z):\n",
    "        return self.decoder(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer encoder_111 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n"
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "The layer has never been called and thus has no defined input shape.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-195-0b0c4c545b86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\myvenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;31m# Eager execution on data tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\myvenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2414\u001b[0m         \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2415\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2416\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint:disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2417\u001b[0m       \u001b[1;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2418\u001b[0m       \u001b[1;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-182-51d3f15a4669>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, batch_input_shape)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"bias\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"zeros\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\myvenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36minput_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \"\"\"\n\u001b[0;32m   1847\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m       raise AttributeError('The layer has never been called '\n\u001b[0m\u001b[0;32m   1849\u001b[0m                            'and thus has no defined input shape.')\n\u001b[0;32m   1850\u001b[0m     all_input_shapes = set(\n",
      "\u001b[1;31mAttributeError\u001b[0m: The layer has never been called and thus has no defined input shape."
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_shape=32, dropout=0.2, reg_cof=0.001, compression=0.8)\n",
    "encoder((trainY[0],))\n",
    "decoder = DenseTranspose(encoder)\n",
    "\n",
    "opt = Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "model = tf.keras.models.Model(encoder, decoder(encoder))\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer encoder_99 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n[<tf.Variable 'encoder_99/dense_99/kernel:0' shape=(32, 25) dtype=float32, numpy=\narray([[-5.62692573e-03, -3.23699345e-03, -9.17229056e-03,\n        -1.12210959e-02, -2.25413218e-03,  3.96442739e-03,\n         4.50964691e-03, -2.18483377e-02, -3.46395164e-03,\n        -2.01933886e-04,  1.26074059e-02, -7.82557018e-03,\n        -5.06875524e-03,  3.59702785e-03,  1.06224036e-02,\n         3.66191985e-03,  2.62866961e-03, -3.60417576e-03,\n        -5.56925125e-03, -1.11007188e-02,  2.86216033e-03,\n        -6.53993711e-03, -2.10929438e-02, -2.04018480e-03,\n         6.66542863e-03],\n       [-2.15256773e-02,  2.25247419e-03,  1.89335383e-02,\n        -2.88081076e-03,  5.44903846e-03,  1.18941907e-03,\n         1.50833344e-02,  8.19897838e-03,  5.00945002e-03,\n        -7.77333789e-03, -7.32019870e-03,  3.60200601e-03,\n         2.54572509e-03, -9.98340733e-03, -6.54304354e-03,\n         1.80905445e-05,  6.47691451e-03,  1.73819028e-02,\n         1.16754258e-02, -7.41700223e-03,  3.77438334e-03,\n         1.12239141e-02, -5.21790842e-03, -1.23719769e-02,\n         2.99099833e-03],\n       [-5.78836445e-03, -9.14296228e-03, -1.45266373e-02,\n        -2.28941403e-02,  3.04553192e-04,  9.66246333e-03,\n         5.63546037e-03,  4.78466274e-03,  1.32985937e-03,\n        -6.26929291e-03, -1.32468175e-02,  4.43335716e-03,\n        -1.29947849e-02,  2.20896839e-03, -5.95075963e-03,\n        -1.07408212e-02, -8.76383565e-04,  8.83766171e-03,\n        -9.52961855e-03, -1.20226033e-02,  1.11297378e-02,\n        -1.54888947e-02,  2.01284396e-03, -2.51273363e-04,\n        -7.19222566e-03],\n       [ 1.14108445e-02, -6.43265760e-03, -7.30463024e-03,\n        -9.48703941e-03,  3.04400292e-03, -7.86406547e-03,\n        -6.86811190e-03, -9.35644284e-03,  1.02761351e-02,\n        -1.14142867e-02, -1.14329411e-02,  1.31639596e-02,\n        -3.19150370e-03,  6.18498959e-03,  7.22081866e-03,\n         6.17211033e-03, -1.63559821e-02, -3.22631584e-03,\n         7.98318768e-04, -7.95888994e-03, -1.91725697e-02,\n        -8.85835290e-03,  1.18509186e-02, -3.30062490e-03,\n         1.39441146e-02],\n       [-1.31837009e-02,  7.95069989e-03,  5.39389718e-03,\n        -2.97181867e-02,  4.25689016e-03, -2.18537841e-02,\n        -1.94691564e-03, -1.69696212e-02, -3.43307527e-03,\n        -7.31466850e-03,  2.22803652e-02, -3.73500655e-03,\n         3.46550718e-03,  9.02699400e-03,  3.14545841e-03,\n        -5.33743668e-03, -2.21715472e-03, -6.37917500e-03,\n         1.96831231e-03, -6.25363924e-03,  3.81660508e-03,\n        -2.01799180e-02,  5.49466163e-03,  4.42639976e-05,\n        -4.41283360e-03],\n       [ 2.50553217e-04,  1.37642389e-02, -7.12552061e-03,\n         1.08745201e-02,  8.79823510e-03,  1.03354212e-02,\n         1.02417897e-02,  5.91088785e-04,  3.27263167e-03,\n         2.08315980e-02, -1.54434573e-02, -2.92464369e-03,\n         1.20143145e-02,  1.47624612e-02, -4.91762161e-03,\n        -8.46646819e-03,  6.49913633e-03, -2.43407604e-03,\n         1.37478365e-02, -2.17014905e-02,  1.89537387e-02,\n         1.57351103e-02, -5.86155662e-03, -1.29706301e-02,\n        -2.39140820e-02],\n       [ 7.85051845e-03, -1.96256628e-03, -6.35519251e-03,\n        -1.41553972e-02,  4.72299429e-03,  1.01365056e-02,\n         6.55749068e-03,  8.68816394e-03,  1.02630267e-02,\n         4.58933599e-03, -5.38540399e-03, -5.63149853e-03,\n         1.73581280e-02, -7.96104199e-04, -6.50218176e-03,\n        -1.99753195e-02,  1.14350086e-02, -6.19535567e-03,\n         7.37053063e-03,  1.19643230e-02, -7.81768095e-03,\n        -3.61359329e-03, -7.46695325e-03, -3.07607977e-03,\n        -1.50185032e-03],\n       [ 2.44049393e-02,  4.55252919e-03,  1.10870991e-02,\n        -8.37768149e-03,  3.33961053e-03,  9.74482484e-03,\n         3.98541288e-03, -2.00871518e-03,  5.40806759e-05,\n         3.69186373e-03, -5.22426842e-03, -2.43689996e-04,\n        -6.23455271e-03,  7.44410558e-03,  1.11293755e-02,\n         7.77379144e-03,  8.30419734e-03, -1.99962333e-02,\n         4.10467107e-03, -3.25366901e-03,  4.83719679e-03,\n         9.80254170e-03,  4.82357386e-03, -3.59408557e-03,\n         1.11555932e-02],\n       [ 3.32182599e-03,  6.87399181e-03,  1.75032821e-02,\n        -3.40073486e-03, -4.16459702e-03,  7.24472152e-03,\n         1.05562303e-02,  1.65021757e-03,  1.23348730e-02,\n        -4.09216527e-03,  1.40423067e-02, -2.36259885e-02,\n         9.62261809e-04, -2.20020339e-02,  1.04365265e-02,\n        -1.25074023e-02,  1.99342482e-02,  2.58481018e-02,\n        -8.72119609e-03,  5.24567347e-03,  7.11883465e-03,\n         7.54690403e-03, -2.81322165e-03, -1.16562294e-02,\n         8.00071470e-03],\n       [ 1.10013383e-02,  1.25502441e-02,  1.66257552e-03,\n         1.24643231e-02, -1.84983425e-02,  1.08436197e-02,\n         1.40338875e-02,  3.75697762e-03,  1.53937573e-02,\n        -7.89094530e-03,  5.30388951e-03, -2.34853737e-02,\n        -1.23343329e-04, -8.45043734e-03, -1.46423129e-03,\n        -6.80503668e-03,  2.23031826e-02,  2.29223389e-02,\n         1.61308818e-03,  1.27239879e-02, -3.49112717e-03,\n         4.17706557e-03,  6.17986964e-03, -9.47308727e-03,\n        -4.68078331e-04],\n       [ 1.17174136e-02,  2.87524369e-02, -1.07129605e-03,\n        -1.21155009e-02,  1.20186657e-02,  1.13312220e-02,\n        -1.71382949e-02, -1.39894302e-03, -1.48733088e-03,\n         1.10761179e-02, -1.69133581e-02, -8.67892173e-04,\n         3.18188104e-03, -1.17681222e-03,  8.59323703e-03,\n         9.80484067e-04,  1.97916012e-03,  5.95683046e-03,\n        -1.66140869e-03, -9.43093840e-03, -6.19318336e-03,\n         1.78375356e-02,  7.08473206e-04, -8.94153025e-03,\n        -9.79085825e-03],\n       [-1.89399777e-03,  4.79102164e-04, -1.55099353e-03,\n        -4.32974147e-03,  5.56311198e-03, -4.57259640e-03,\n        -1.15561271e-02, -5.84554905e-03, -4.13283147e-03,\n         1.03417400e-03,  4.29424923e-03, -1.21525535e-02,\n        -1.01648911e-03,  4.85998625e-03, -8.35633837e-03,\n        -1.78851485e-02, -1.12906964e-02, -1.51446126e-02,\n         2.01050602e-02,  5.69778681e-03, -1.27367377e-02,\n        -6.23710454e-03, -1.28907701e-02, -1.02466922e-02,\n         1.56778768e-02],\n       [ 1.86261125e-02,  3.23764351e-03,  5.85149901e-05,\n        -6.89608452e-04, -4.31120908e-03,  4.14721342e-03,\n        -4.47375281e-03,  2.33920221e-03,  1.27146505e-02,\n        -4.23135655e-03,  9.35590360e-03,  6.31414168e-03,\n         1.87225780e-03,  8.55182298e-03,  1.62307378e-02,\n         5.40080015e-03,  9.29615868e-04, -2.67437659e-03,\n         7.59711862e-03, -8.90278351e-03, -2.57176533e-03,\n         9.52127110e-03,  1.16981873e-02,  1.60369687e-02,\n         4.53123637e-03],\n       [ 3.02074477e-04, -4.82306676e-03, -2.18659826e-03,\n         3.79275065e-03, -1.91483879e-03,  5.45987068e-03,\n         3.71203828e-03, -7.99257215e-03,  5.83622931e-03,\n        -1.57893430e-02,  1.12350835e-02,  1.02379806e-02,\n         6.29330752e-03, -8.95040017e-03, -1.57078225e-02,\n         9.67543479e-03,  2.18484434e-03, -7.84096424e-04,\n        -7.42983166e-03,  6.91272225e-03,  3.57113108e-02,\n        -4.31621401e-03,  6.65813498e-03,  1.08395787e-02,\n        -7.38760177e-03],\n       [-1.70361269e-02,  1.35247083e-02, -6.91760145e-03,\n         1.68618076e-02, -1.44182546e-02,  4.60290263e-04,\n        -2.58959038e-03,  1.80801488e-02,  2.00970564e-03,\n        -1.03250667e-02, -4.12993971e-03, -1.01235425e-02,\n         1.52881257e-02, -1.60461143e-02, -6.51617441e-03,\n         2.59104627e-03, -2.59323348e-03,  6.81009330e-03,\n        -1.52404923e-02, -7.53228413e-03,  2.84483633e-03,\n         3.27325449e-03,  2.61104805e-03,  1.25276744e-02,\n         1.91721264e-02],\n       [ 4.75699082e-03, -4.92390664e-03, -7.19964373e-05,\n        -1.82180584e-03,  7.67676672e-03, -6.45413902e-03,\n        -2.81529571e-03,  4.67441045e-03, -4.94720694e-03,\n        -5.38356893e-04,  4.75955289e-03, -3.16308765e-03,\n        -7.58819189e-03, -1.62330980e-04,  4.20033466e-03,\n         1.07853990e-02,  1.07211899e-02, -1.62775978e-04,\n         1.98299414e-03,  5.76326903e-03, -4.58320463e-03,\n        -1.21612931e-02,  7.30101252e-03,  8.04805756e-03,\n        -1.10480906e-02],\n       [-1.90893915e-02,  1.02534257e-02,  8.07726756e-03,\n        -1.01631843e-02,  1.27746398e-02, -1.03770932e-02,\n         3.24893440e-03,  9.43223364e-04, -4.10959963e-03,\n        -1.59923173e-02, -3.41442041e-03,  3.92957532e-04,\n        -2.33670580e-04, -4.15622629e-03, -1.15660522e-02,\n         6.81086583e-03, -1.63119249e-02, -1.67461083e-04,\n         1.16366716e-02, -9.27548390e-03, -8.51757731e-03,\n        -1.70130897e-02, -1.94724882e-03, -1.09195206e-02,\n        -2.05380842e-02],\n       [-1.46499043e-02, -3.90261528e-03, -1.10839412e-03,\n         2.15220125e-03,  6.06322056e-03,  9.44866426e-03,\n        -5.76723251e-05, -2.35438012e-02,  1.45461992e-03,\n         1.42110456e-02, -7.31055206e-03,  2.99925823e-03,\n        -3.53890937e-03, -1.06034912e-02, -1.64031200e-02,\n        -1.35516124e-02, -7.82513700e-04,  5.31845260e-03,\n         6.80999225e-03, -1.22037763e-02, -7.13858102e-03,\n         2.84534283e-02, -6.47592358e-03, -6.57149707e-04,\n        -7.10550044e-03],\n       [-4.93344851e-03,  1.02509642e-02, -4.61845566e-03,\n        -1.04741435e-02, -6.14787079e-03, -8.66599102e-03,\n        -2.60965247e-03,  1.62504464e-02,  1.12121003e-02,\n         3.48491035e-03, -9.90680978e-03, -8.93563684e-03,\n         1.83563232e-02, -1.50335645e-02, -2.31496450e-02,\n         4.60503995e-03, -8.96446873e-03, -2.98020826e-03,\n        -8.56287871e-03, -1.49899600e-02, -1.25091439e-02,\n         1.73956645e-03,  1.20228510e-02, -8.48721527e-03,\n         9.97779891e-04],\n       [ 4.07479005e-03, -6.65448420e-03,  7.73886731e-03,\n        -1.04009099e-02, -1.50786042e-02, -1.86095629e-02,\n        -3.53747816e-03, -4.88098571e-03,  2.03651981e-03,\n        -7.70212198e-03,  5.56899374e-03, -5.38222957e-03,\n         1.46821970e-02,  1.73236951e-02, -2.19149585e-03,\n        -4.74527152e-03, -5.69596700e-03, -8.90222192e-03,\n         6.00155536e-03, -9.97547992e-03, -5.04719419e-03,\n        -3.94556159e-03,  1.95076235e-03,  3.85683816e-04,\n         2.86365976e-03],\n       [ 5.65696647e-03, -8.57991539e-03,  9.28179268e-03,\n        -9.59088933e-03, -1.30667910e-02,  1.35700952e-03,\n        -9.11312457e-03,  7.77716562e-03,  7.46520190e-03,\n         8.84477794e-03,  1.55596845e-02, -9.69079090e-04,\n        -1.95171051e-02, -9.60867200e-03, -5.02053648e-04,\n        -8.55657272e-03,  9.36199538e-03, -3.21432459e-03,\n        -1.20582152e-02,  8.28167703e-03, -1.33739179e-02,\n         6.79703755e-03, -1.62790529e-03,  1.70705430e-02,\n         5.27303014e-03],\n       [ 1.55852446e-02,  4.91233449e-03,  1.24019338e-03,\n        -1.36939650e-02, -6.03080122e-03, -1.17904758e-02,\n         3.84619995e-03,  1.55699300e-02,  3.93930264e-03,\n        -1.45680308e-02, -1.01111867e-02, -7.95193203e-03,\n         2.41798954e-03,  1.20732570e-02,  2.02819775e-03,\n        -4.54392517e-03,  7.02690240e-03, -1.59455445e-02,\n        -2.38336669e-03, -2.82913377e-03,  6.69035828e-03,\n         1.23414565e-02, -2.79488415e-03, -2.26727072e-02,\n         7.62927020e-03],\n       [ 1.04392525e-02, -3.88219283e-04,  1.42018283e-02,\n        -1.59457326e-02,  5.36392396e-03,  8.45133327e-03,\n        -8.35615490e-03, -5.15688851e-04, -1.02819747e-03,\n        -7.30320625e-03,  2.67789816e-03,  3.86358192e-03,\n         7.08593871e-04, -1.33734178e-02,  4.04489320e-03,\n         1.14593739e-02, -2.70662177e-03,  1.16306711e-02,\n        -1.55926533e-02,  1.40797475e-03,  2.80482881e-03,\n         7.43284181e-04,  4.83023794e-03,  1.22828083e-02,\n        -2.19714828e-02],\n       [ 3.04520270e-03,  1.86564736e-02, -1.63326587e-03,\n        -6.66602049e-03,  2.72091641e-03,  9.99168213e-03,\n        -1.29980547e-03,  3.88729200e-03, -7.64358137e-03,\n         1.08285500e-02,  4.73082997e-03,  1.08074127e-02,\n         7.22960322e-05, -3.53169465e-03,  2.30649882e-03,\n         1.91165949e-03,  2.43206415e-03, -8.44899379e-03,\n        -9.16907564e-03, -7.37415021e-03, -9.03235935e-03,\n        -5.22141857e-03, -1.56792980e-02, -1.76276185e-03,\n        -9.30010062e-03],\n       [ 1.28089497e-02, -6.91603310e-03,  1.27215693e-02,\n         9.15792491e-03,  8.80995300e-03,  1.81420576e-02,\n         2.20297650e-03,  2.21702037e-03,  1.37100909e-02,\n        -1.66365947e-03, -1.87899228e-02, -1.16594281e-04,\n         8.11211672e-03,  1.01793939e-02,  8.08682758e-03,\n        -8.88615008e-03, -4.35618497e-03,  9.12647601e-03,\n         9.50969663e-03, -4.18660138e-03,  1.29868435e-02,\n        -3.17916810e-03,  1.73351578e-02, -2.90329917e-03,\n        -7.80890044e-03],\n       [ 4.21340344e-03, -3.80561833e-04, -9.00527555e-03,\n         1.38869081e-02,  8.57598800e-03,  2.19014529e-02,\n         1.00385901e-02, -1.22005399e-02, -1.48740541e-02,\n         1.50919915e-03,  1.19508719e-02, -1.70738320e-03,\n        -5.63566398e-04,  6.35515014e-03, -2.14829184e-02,\n        -1.51549531e-02, -8.60191602e-03,  2.01695575e-03,\n        -4.44283430e-03,  7.38173397e-03, -9.73114278e-03,\n        -1.64064784e-02, -3.49856960e-03, -3.39469593e-03,\n         3.31524014e-03],\n       [ 4.18687519e-03, -4.68231738e-03,  8.86888988e-03,\n        -1.57321058e-02,  1.07804593e-02, -6.12180401e-03,\n        -3.05022090e-03, -3.57238157e-03,  3.03506455e-03,\n         3.72235943e-03, -5.19019179e-03,  7.46703241e-03,\n         3.27826128e-03, -1.10479174e-02,  3.78783647e-04,\n        -7.49729341e-03, -4.42461902e-03,  9.83532332e-03,\n        -5.33615937e-03,  5.84654370e-03, -1.50488259e-03,\n        -1.22718303e-03, -1.36417504e-02, -3.12813483e-02,\n        -1.73472911e-02],\n       [ 3.33407661e-03, -1.27761047e-02, -7.59367039e-03,\n         6.45233924e-03,  2.43804549e-04,  3.05007445e-03,\n        -1.57796945e-02, -9.29945894e-03,  1.81768220e-02,\n         8.54943041e-03,  2.08928555e-04,  6.48888946e-03,\n         1.93055179e-02,  3.40086245e-03, -9.13463824e-04,\n        -2.92181061e-03,  3.45865032e-03,  5.61328651e-03,\n        -5.30067412e-03, -1.17664998e-02,  1.29855750e-03,\n         1.86409876e-02,  2.75073247e-03,  2.99697625e-03,\n        -6.15137396e-03],\n       [ 3.50798247e-03, -1.56013565e-02,  6.51433691e-03,\n         3.13838781e-03, -1.46395830e-03,  2.97674420e-03,\n        -1.46048935e-02,  2.64001987e-03, -1.12414071e-02,\n        -1.07166013e-02, -1.73737717e-04, -4.03607776e-03,\n         6.70666061e-03,  9.04641114e-03,  2.39969753e-02,\n         9.76113789e-03,  7.65248295e-03,  2.07588600e-04,\n         1.92705635e-02,  1.84473151e-03, -5.81639772e-03,\n        -6.68729842e-03, -1.10244928e-02,  1.19491632e-03,\n        -1.77180842e-02],\n       [-1.29417358e-02, -1.39989983e-02, -3.31660057e-03,\n        -7.37431739e-03,  1.16818892e-02, -1.11063821e-02,\n         3.37284827e-03,  7.46122980e-03,  1.09593403e-02,\n         3.12264217e-03,  8.57062230e-04,  4.91832802e-03,\n        -1.10618267e-02,  1.30291097e-02,  2.14380492e-02,\n        -2.53123930e-03, -1.22955861e-02, -2.18151300e-03,\n         6.22249721e-03, -9.16234311e-03,  1.71313330e-03,\n        -1.59891080e-02,  1.75828096e-02,  6.60136633e-04,\n         4.34802147e-03],\n       [-1.22545231e-02, -1.71369705e-02, -1.58455188e-03,\n         7.20190769e-03,  2.96187447e-03, -2.99881608e-03,\n        -5.15331747e-04,  6.45112619e-03,  9.31536965e-03,\n         8.43048748e-03,  5.50364889e-03, -1.44732092e-02,\n        -2.83343922e-02,  1.46089168e-02, -2.12003961e-02,\n         8.14823492e-04, -8.95118108e-04, -1.44557497e-02,\n        -3.90349678e-03, -8.87797587e-03,  7.24581443e-03,\n        -1.66255217e-02, -2.55376799e-03,  8.41687620e-03,\n        -4.03929083e-03],\n       [ 1.66111754e-03, -9.01645608e-03,  4.81286971e-03,\n         1.10342717e-02, -1.85091216e-02,  9.76647018e-04,\n        -4.98522352e-03,  7.26292050e-03, -1.51140569e-03,\n        -6.54189964e-04,  6.37971284e-03, -1.34270359e-02,\n        -1.20590562e-02,  2.76645180e-02, -1.80604160e-02,\n        -3.42021370e-03,  1.43114422e-02,  1.30694862e-02,\n         1.24469411e-03, -9.86735243e-03,  6.59460761e-03,\n        -1.15033863e-02,  2.93683335e-02,  2.43809586e-03,\n        -1.37523497e-02]], dtype=float32)>, <tf.Variable 'encoder_99/dense_99/bias:0' shape=(25,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]\n"
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "The layer has never been called and thus has no defined input shape.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-183-0a812ca93134>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m model.fit(\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-174-bbf3373c5888>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, compression, dropout, reg_cof, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             ) \n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-182-51d3f15a4669>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-182-51d3f15a4669>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, batch_input_shape)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"bias\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"zeros\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\myvenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36minput_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \"\"\"\n\u001b[0;32m   1847\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m       raise AttributeError('The layer has never been called '\n\u001b[0m\u001b[0;32m   1849\u001b[0m                            'and thus has no defined input shape.')\n\u001b[0;32m   1850\u001b[0m     all_input_shapes = set(\n",
      "\u001b[1;31mAttributeError\u001b[0m: The layer has never been called and thus has no defined input shape."
     ]
    }
   ],
   "source": [
    "model = Autoencoder()\n",
    "opt = Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "model.fit(\n",
    "    trainY, trainY, \n",
    "    epochs =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(32,)"
     },
     "metadata": {},
     "execution_count": 302
    }
   ],
   "source": [
    "import util as ut\n",
    "trainY = ut.generate_training_sat(32,100)[1]\n",
    "np.shape(trainY[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_95\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_157 (InputLayer)       [(None, 32)]              0         \n_________________________________________________________________\ndropout_133 (Dropout)        (None, 32)                0         \n_________________________________________________________________\ndense_133 (Dense)            (None, 25)                825       \n_________________________________________________________________\ndense_transpose_73 (DenseTra (None, 32)                857       \n=================================================================\nTotal params: 857\nTrainable params: 857\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/2\nFor batch 0, loss is    0.50.\n  1/100 [..............................] - ETA: 0s - loss: 0.5033For batch 1, loss is    0.55.\nFor batch 2, loss is    0.61.\nFor batch 3, loss is    0.60.\nFor batch 4, loss is    0.59.\nFor batch 5, loss is    0.57.\nFor batch 6, loss is    0.54.\nFor batch 7, loss is    0.54.\nFor batch 8, loss is    0.52.\nFor batch 9, loss is    0.51.\nFor batch 10, loss is    0.50.\nFor batch 11, loss is    0.51.\nFor batch 12, loss is    0.50.\nFor batch 13, loss is    0.50.\nFor batch 14, loss is    0.50.\nFor batch 15, loss is    0.50.\nFor batch 16, loss is    0.49.\nFor batch 17, loss is    0.49.\nFor batch 18, loss is    0.48.\nFor batch 19, loss is    0.48.\nFor batch 20, loss is    0.48.\nFor batch 21, loss is    0.48.\nFor batch 22, loss is    0.47.\nFor batch 23, loss is    0.47.\n 24/100 [======>.......................] - ETA: 0s - loss: 0.4710For batch 24, loss is    0.47.\nFor batch 25, loss is    0.46.\nFor batch 26, loss is    0.46.\nFor batch 27, loss is    0.45.\nFor batch 28, loss is    0.45.\nFor batch 29, loss is    0.45.\nFor batch 30, loss is    0.45.\nFor batch 31, loss is    0.45.\nFor batch 32, loss is    0.44.\nFor batch 33, loss is    0.45.\nFor batch 34, loss is    0.44.\nFor batch 35, loss is    0.44.\nFor batch 36, loss is    0.44.\nFor batch 37, loss is    0.44.\nFor batch 38, loss is    0.44.\nFor batch 39, loss is    0.44.\nFor batch 40, loss is    0.44.\nFor batch 41, loss is    0.43.\nFor batch 42, loss is    0.43.\nFor batch 43, loss is    0.43.\nFor batch 44, loss is    0.43.\nFor batch 45, loss is    0.43.\nFor batch 46, loss is    0.43.\n 47/100 [=============>................] - ETA: 0s - loss: 0.4255For batch 47, loss is    0.42.\nFor batch 48, loss is    0.42.\nFor batch 49, loss is    0.42.\nFor batch 50, loss is    0.42.\nFor batch 51, loss is    0.42.\nFor batch 52, loss is    0.42.\nFor batch 53, loss is    0.42.\nFor batch 54, loss is    0.42.\nFor batch 55, loss is    0.41.\nFor batch 56, loss is    0.41.\nFor batch 57, loss is    0.41.\nFor batch 58, loss is    0.41.\nFor batch 59, loss is    0.41.\nFor batch 60, loss is    0.41.\nFor batch 61, loss is    0.41.\nFor batch 62, loss is    0.41.\nFor batch 63, loss is    0.41.\nFor batch 64, loss is    0.41.\nFor batch 65, loss is    0.41.\nFor batch 66, loss is    0.41.\nFor batch 67, loss is    0.41.\nFor batch 68, loss is    0.41.\nFor batch 69, loss is    0.41.\nFor batch 70, loss is    0.41.\nFor batch 71, loss is    0.40.\nFor batch 72, loss is    0.40.\n 73/100 [====================>.........] - ETA: 0s - loss: 0.4040For batch 73, loss is    0.40.\nFor batch 74, loss is    0.40.\nFor batch 75, loss is    0.40.\nFor batch 76, loss is    0.40.\nFor batch 77, loss is    0.40.\nFor batch 78, loss is    0.40.\nFor batch 79, loss is    0.40.\nFor batch 80, loss is    0.40.\nFor batch 81, loss is    0.40.\nFor batch 82, loss is    0.40.\nFor batch 83, loss is    0.40.\nFor batch 84, loss is    0.40.\nFor batch 85, loss is    0.39.\nFor batch 86, loss is    0.39.\nFor batch 87, loss is    0.39.\nFor batch 88, loss is    0.39.\nFor batch 89, loss is    0.39.\nFor batch 90, loss is    0.39.\nFor batch 91, loss is    0.39.\nFor batch 92, loss is    0.39.\nFor batch 93, loss is    0.39.\nFor batch 94, loss is    0.39.\nFor batch 95, loss is    0.39.\nFor batch 96, loss is    0.39.\n 97/100 [============================>.] - ETA: 0s - loss: 0.3870For batch 97, loss is    0.39.\nFor batch 98, loss is    0.39.\nFor batch 99, loss is    0.39.\n"
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-354-266f3f2c32ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLossAndErrorPrintingCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m )\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\myvenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\myvenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m           \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\envs\\myvenv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-352-21d191c695bb>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The average loss for epoch {} is {:7.2f} and mean absolute error is {:7.2f}.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "sample_space = 10\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "model = shallowNet.build()\n",
    "opt = Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "model.fit(\n",
    "    trainY, trainY, \n",
    "    epochs = 2, \n",
    "    batch_size = 1,\n",
    "    callbacks=[LossAndErrorPrintingCallback()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_74\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_114 (InputLayer)       [(None, 32)]              0         \n_________________________________________________________________\ndropout_113 (Dropout)        (None, 32)                0         \n_________________________________________________________________\ndense_113 (Dense)            (None, 25)                825       \n=================================================================\nTotal params: 825\nTrainable params: 825\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"model_75\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_137 (InputLayer)       [(None, 25)]              0         \n_________________________________________________________________\ndense_transpose_53 (DenseTra (None, 32)                857       \n=================================================================\nTotal params: 857\nTrainable params: 857\nNon-trainable params: 0\n_________________________________________________________________\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'dense_transpose_53_8/Identity:0' shape=(None, 32) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 290
    }
   ],
   "source": [
    "features_list_encoder = [layer.output for layer in model.layers[:3]]\n",
    "encoder = tf.keras.Model(inputs = model.input, outputs = features_list_encoder)\n",
    "features_list_decoder = model.layers[3]# [layer.output for layer in model.layers[3]]\n",
    "inputs = Input(encoder.layers[-1].output_shape[-1])\n",
    "features_list_decoder(inputs)\n",
    "decoder = tf.keras.Model( inputs = inputs ,outputs = features_list_decoder(inputs))\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "decoder.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d0fb334a29fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmse_loss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "epochs =2\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epochs,))\n",
    "    # iterate over the batches of the dataset \n",
    "    for  x_batch_train in trainY:\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = encoder((x_batch_train,))\n",
    "            print(reconstructed)\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            los += sum(model.losses)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 1 == 0:\n",
    "            print('step %s: mean loss = %s' % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitdea3ae2f75b54f458b389d46128237c9",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}